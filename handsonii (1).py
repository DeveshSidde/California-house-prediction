# -*- coding: utf-8 -*-
"""HANDSONII.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HHxZMUFvF-9wMpIZvnrY5xrue5fPech
"""

import pandas as pd
data_path = '/content/housing.csv'
housing_data = pd.read_csv(data_path)
housing_data.head(), housing_data.describe()
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
housing_data_numeric = housing_data.drop('ocean_proximity', axis=1)  # Remove non-numeric data for imputation
imputed_data = imputer.fit_transform(housing_data_numeric)
housing_data_imputed = pd.DataFrame(imputed_data, columns=housing_data_numeric.columns)
X = housing_data_imputed.drop('median_house_value', axis=1)
y = housing_data_imputed['median_house_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train)
y_pred_lin = lin_reg.predict(X_test_scaled)
mse_lin = mean_squared_error(y_test, y_pred_lin)
svr_reg = SVR(kernel='linear')
svr_reg.fit(X_train_scaled, y_train)
y_pred_svr = svr_reg.predict(X_test_scaled)
mse_svr = mean_squared_error(y_test, y_pred_svr)
mse_lin, mse_svr

"""Since the MSE value of the Linear Regression Model is lesser than that of SVR  model, Linear Redression Model is best out of these two."""

import numpy as np
from sklearn.preprocessing import LabelEncoder
bins = [0, 180000, 360000, np.inf]
labels = ['Low', 'Medium', 'High']
housing_data_imputed['house_value_category'] = pd.cut(housing_data_imputed['median_house_value'], bins=bins, labels=labels)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(housing_data_imputed['house_value_category'])
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y_encoded, test_size=0.2, random_state=42)
X_train_clf_scaled = scaler.transform(X_train_clf)
X_test_clf_scaled = scaler.transform(X_test_clf)
X_train_clf_scaled.shape, X_test_clf_scaled.shape, y_train_clf.shape, y_test_clf.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
dt_clf = DecisionTreeClassifier(random_state=42)
dt_clf.fit(X_train_clf_scaled, y_train_clf)
y_pred_dt = dt_clf.predict(X_test_clf_scaled)
accuracy_dt = accuracy_score(y_test_clf, y_pred_dt)
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train_clf_scaled, y_train_clf)
y_pred_rf = rf_clf.predict(X_test_clf_scaled)
accuracy_rf = accuracy_score(y_test_clf, y_pred_rf)
accuracy_dt, accuracy_rf

"""From the accuracy of the models, We can say that the Random Forest Classifier is the best out of the two."""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
data = pd.read_csv('/content/housing.csv')
features = data[['longitude', 'latitude']]
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
inertias = []
k_range = range(1, 11)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(features_scaled)
    inertias.append(kmeans.inertia_)
plt.figure(figsize=(8, 4))
plt.plot(k_range, inertias, marker='o')
plt.title('Elbow Method for Determining Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(features_scaled)
data['Cluster_ID'] = clusters
plt.figure(figsize=(8, 6))
plt.scatter(data.loc[data['Cluster_ID'] == 0, 'longitude'], data.loc[data['Cluster_ID'] == 0, 'latitude'], s=50, c='red', label='Cluster 1') # Using 'longitude' and 'latitude' for visualization
plt.scatter(data.loc[data['Cluster_ID'] == 1, 'longitude'], data.loc[data['Cluster_ID'] == 1, 'latitude'], s=50, c='blue', label='Cluster 2') # Using 'longitude' and 'latitude' for visualization
plt.scatter(data.loc[data['Cluster_ID'] == 2, 'longitude'], data.loc[data['Cluster_ID'] == 2, 'latitude'], s=50, c='green', label='Cluster 3') # Using 'longitude' and 'latitude' for visualization
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='yellow', marker='*', label='Centroids')
plt.title('Data Clusters and Centroids')
plt.xlabel('longitude')
plt.ylabel('latitude')
plt.legend()
plt.show()